{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a9ada1-e2d5-4832-8dd2-6986ce866904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c531d8e2-ccfc-4847-9611-e66fab81dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # input_size: The number of features in the input \n",
    "        # at each time step.\n",
    "        # hidden_size: The number of features in the hidden\n",
    "        # state (also referred to as the number of hidden units).\n",
    "        # Number of features in the hidden state, which determines \n",
    "        # how much memory the RNN has at each time step.\n",
    "        # output_size: The number of features in the final \n",
    "        # output (this could be the number of classes for \n",
    "        # classification or the dimension of the output for \n",
    "        # regression).\n",
    "        \n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Defining the layers\n",
    "        # batch_first=True: This means the input tensor will \n",
    "        # have shape (batch_size, seq_len, input_size). If \n",
    "        # batch_first is False (the default), the input shape \n",
    "        # would be (seq_len, batch_size, input_size). \n",
    "        # Specifying batch_first=True makes it more convenient \n",
    "        # to use, as batch size is often the first dimension \n",
    "        # in data processing.\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for the first time step\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # 1: The RNN is unidirectional and has 1 layer, \n",
    "        # so the first dimension represents the number of \n",
    "        # layers in the RNN (which is 1 in this case).\n",
    "        # x.size(0): This gives the batch size (the number \n",
    "        # of sequences being processed in parallel). This \n",
    "        # ensures that each sequence has its own hidden state.\n",
    "        # self.hidden_size: This is the number of features in \n",
    "        # the hidden state, as defined earlier in the constructor.\n",
    "        \n",
    "        # Passing the input and hidden state through the RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        # This passes the input tensor x and the initial hidden \n",
    "        # state h0 through the RNN:\n",
    "        # out: The output from the RNN for every time step \n",
    "        # (shape: (batch_size, seq_len, hidden_size)).\n",
    "        # hn: The final hidden state at the last time step \n",
    "        # (shape: (num_layers, batch_size, hidden_size)), \n",
    "        # which can be used to initialize the hidden state\n",
    "        # for the next batch of sequences (if training \n",
    "        # statefully, which isn't happening here).\n",
    "        \n",
    "        # Getting the last time step's output\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        #Here, we take the output from the last time step \n",
    "        # for each sequence in the batch:\n",
    "        # out[:, -1, :] means: \"for every sequence in \n",
    "        # the batch (:), take the output of the last \n",
    "        # time step (-1), and take all features in \n",
    "        # the hidden state (:)\".\n",
    "        # This gives us the output corresponding to the \n",
    "        # final time step of each sequence, which is often \n",
    "        # what you want when performing tasks like sequence\n",
    "        # classification or regression.\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f813b855-e426-4a65-b850-732fe0429601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 10  # Number of features in input\n",
    "hidden_size = 20 # Number of hidden units in RNN\n",
    "output_size = 1  # Output size (e.g., for regression)\n",
    "num_epochs = 100  # Number of training epochs\n",
    "learning_rate = 0.001\n",
    "sequence_length = 7  # Length of the input sequence\n",
    "batch_size = 5  # Number of samples in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd6fab0-e78a-4431-bf7b-e76e9aef1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model instance\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffeeda0e-f2d7-4e06-a955-63e3238e20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for training\n",
    "# Here, we create random inputs (X) and corresponding random outputs (y)\n",
    "# X: (batch_size, sequence_length, input_size)\n",
    "# y: (batch_size, output_size)\n",
    "def generate_synthetic_data(batch_size, sequence_length, input_size):\n",
    "    X = torch.randn(batch_size, sequence_length, input_size)  # Random input tensor\n",
    "    y = torch.randn(batch_size, output_size)  # Random target output\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c9be2a-f4c5-410d-b357-d40f904be6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.3734\n",
      "Epoch [20/100], Loss: 0.2098\n",
      "Epoch [30/100], Loss: 0.0985\n",
      "Epoch [40/100], Loss: 0.0345\n",
      "Epoch [50/100], Loss: 0.0067\n",
      "Epoch [60/100], Loss: 0.0001\n",
      "Epoch [70/100], Loss: 0.0009\n",
      "Epoch [80/100], Loss: 0.0005\n",
      "Epoch [90/100], Loss: 0.0001\n",
      "Epoch [100/100], Loss: 0.0000\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Get synthetic data\n",
    "inputs, targets = generate_synthetic_data(\n",
    "    batch_size, sequence_length, input_size)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Clear the gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b3fd1f-8994-4bae-b985-a80b600747d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([[-0.5498],\n",
      "        [-0.1748],\n",
      "        [-0.0134],\n",
      "        [-0.2525],\n",
      "        [ 0.1523]])\n"
     ]
    }
   ],
   "source": [
    "# Generate new synthetic data for testing/prediction\n",
    "new_input, y = generate_synthetic_data(\n",
    "    batch_size, sequence_length, input_size)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():  # Disable gradient computation since we're only doing inference\n",
    "    predictions = model(new_input)\n",
    "    \n",
    "print(f\"Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a38d00-55b4-4304-9c03-568359f33605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5991],\n",
       "        [ 0.0954],\n",
       "        [-0.0533],\n",
       "        [ 1.3177],\n",
       "        [-1.0983]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac3a84-8b5c-4cd2-b03f-9289af979dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
